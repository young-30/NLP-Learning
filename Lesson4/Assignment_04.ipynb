{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 复习上课内容以及复现课程代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本部分，你需要复习上课内容和课程代码后，自己复现课程代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does a neuron compute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans[1]:神经元节点包含输入与输出，将输入的节点通过本节点的计算方式生成本节点的值进行前向传播，并将本节点的梯度结合输出节点传回的梯度结合\n",
    "       进行反向传播。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Why we use non-linear activation funcitons in neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans[2]:如果使用线性函数作为激活函数的话，神经网络的深度就没有意义了，因为不论多少层，都可以最终用一层线性函数来代替。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the 'Logistic Loss' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans[3]:'Logistic Loss' 是二分类问题的cost function，使用真实值与预测值作为输入，利用log函数计算loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "\n",
    "A. ReLU    \n",
    "B. Leaky ReLU    \n",
    "C. sigmoid    \n",
    "D. tanh  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans[4]:C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Why we don't use zero initialization for all parameters ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans[5]:因为神经网络存在隐藏层，如果将权值参数w初始化为0，经过前向传播到隐藏层时，通过激活函数计算得到的不同神经元的值可以认为是相似(相差一个b常数)\n",
    "       甚至是相同的(b也初始化为0)，这时在反向传播过程中，我们使用梯度下降的方式来降低损失函数，在更新权值的过程中，代价函数对不同权值参数的偏导数相同,\n",
    "        无论更新多少轮，参数也没有产生迭代变化，神经网络也就失去了学习能力。\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you implement the softmax function using python ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_1(x):\n",
    "    e = np.exp(x)\n",
    "    x_sum = np.sum(e,axis=1)[:, np.newaxis] # 对行求和\n",
    "    b = np.tile(x_sum,(1,np.shape(x)[1])) # 填充为对应shape的sum值数组\n",
    "    ret = e/b\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this practical part, you will build a simple digits recognizer to check if the digit in the image is larger than 5. This assignmnet will guide you step by step to finish your first small project in this course ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Packages  \n",
    "sklearn is a famous package for machine learning.   \n",
    "matplotlib is a common package for vasualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Overview of the dataset  \n",
    "    - a training set has m_train images labeled as 0 if the digit < 5 or 1 if the digit >= 5\n",
    "    - a test set contains m_test images labels as if the digit < 5 or 1 if the digit >= 5\n",
    "    - eah image if of shape (num_px, num_px ). Thus, each image is square(height=num_px and  width = num_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data \n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADWCAYAAADmbvjqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAD6JJREFUeJzt3X+MVnV2x/HPKSwuah0QqKlgHYwNWbKGHxJrayKwi427bRbaRLObbAOmCaSxDZAmhf4l/gdJ08AfTUOjdUi6dQPuLmyaZrsYB7qbtLYgQ6tlSRGGBVx/EIS1rakrPf1jxkTd+Z77PHdmnu8h834lRPA8z9wzX+79eHk4fq+5uwAA9f1C7QYAACMIZABIgkAGgCQIZABIgkAGgCQIZABIgkAGgCQIZABIgkAGgCSmd/PiuXPnen9/f9cHeffdd8P6xYsXi7XbbrutWFuwYEGxNm3atObGxjA8PKzLly9bp69vuyZNTp8+Xaxdv369WLvzzjuLtVmzZrXu5/jx45fdfV4nr52sNXnvvfeKtddff71YmzlzZrG2aNGi1v10syZS+3V58803w/qlS5eKtRkzZhRrixcvLtZu9OsnukbOnTtXrN17770T3ovU+bnSVSD39/fr2LFjXTdz4MCBsL5t27Zi7ZFHHinWdu7cWazNnj27ubExrFixoqvXt12TJqtWrSrWrl69Wqw9/fTTxdratWtb92Nm5zt97WStyZEjR4q1devWFWtLly5t9TWbdLMmUvt12bVrV1jfvn17sTZ//vxi7aWXXirWbvTrJ7pGNmzYUKwdPHhwwnuROj9X+MgCAJIgkAEgCQIZAJIgkAEgCQIZAJLoasqirWiKQorHUKKRudtvv71Y279/f3jMxx57LKzXFo2oHT16tFgbHBws1sYzZdELQ0NDYX316tXFWl9fX7E2PDzctqWeiSYlms7lvXv3FmubNm0q1o4fP16srVmzJjxmdgMDA8VaNHVTG3fIAJAEgQwASRDIAJAEgQwASRDIAJAEgQwASUzY2Fs0QhONtUnxTl333HNPsRZtPBT1I9Ufe2sa8Wq76U3mkZ4mTRu7LFmypFiLNheKNlzKYuPGjcVa09jo/fffX6wtXLiwWLuRR9uizYOkeOxty5Ytxdp4RiQnYtc67pABIAkCGQCSIJABIAkCGQCSIJABIAkCGQCSIJABIIkJm0OOtslcvnx5+N5o1jgSzV9msHv37mJtx44d4XuvXbvW6pjRw1Gzi+ZDpXjOM3pv9m1HpfgaOHv2bPjeaM4/mjWOrtm2DzntlWjOWIrniaOHnEbnUdNT25uu6U5whwwASRDIAJAEgQwASRDIAJAEgQwASRDIAJBET8beom0yJ+uYGcZ2ohGaaPRGat9/07aEtUX9RWOCUvP2nCVNI1LZNY2FXrlypViLxt6i2osvvhgesxfX16FDh4q1rVu3hu9dv359q2Pu2bOnWHvuuedafc1ucIcMAEkQyACQBIEMAEkQyACQBIEMAEkQyACQxISNvUVjME1PgI5Eo23Hjh0r1h5//PHWx7yRRU+zzvBE6mhHrGjkqEk0Ete0S9eNLrr2ovG1TZs2FWu7du0Kj7lz587mxsapr6+vVU2S9u3bV6w1PfG9JHqy+UThDhkAkiCQASAJAhkAkiCQASAJAhkAkiCQASCJCRt7i3akisbTJOnAgQOtapFt27a1eh8mV7TL3ZEjR8L3njx5sliLRpKih5w+8cQT4TEzPCB1+/btYb3tg0wPHz5crGUYG40e2Nu0q2E02hZ93WiXuF6MT3KHDABJEMgAkASBDABJEMgAkASBDABJEMgAkASBDABJ9GQOuWkrv2hmeMWKFcXaeLb1rK1ppjGaf42exhvN8jY96boXoi1Am7ZFjOrRtp7RevX394fHzDCH3PSE540bN7b6utGs8d69e1t9zSyi6+vatWvFWu1rhDtkAEiCQAaAJAhkAEiCQAaAJAhkAEiCQAaAJMzdO3+x2TuSzk9eOync7e7zOn3xFFkTqYt1YU3GNkXWhTUZW0fr0lUgAwAmDx9ZAEASBDIAJEEgA0ASBDIAJEEgA0ASBDIAJEEgA0ASBDIAJJE2kM3sUTM7bWZnzGx77X5qM7O/NrO3zezV2r1kYWZ3mdmgmZ0ys9fMbHPtnmozs8+a2b+Y2cnRNXm6dk9ZmNk0MzthZn9Xu5eSlIFsZtMk/YWkL0laLOlrZra4blfVDUh6tHYTyXwo6Y/d/XOSHpT0JOeJ/lfSF9x9iaSlkh41swcr95TFZkmnajcRSRnIkh6QdMbdz7r7B5K+Kan+s3Qqcvd/lHSldh+ZuPtP3P2V0Z+/p5GLbX7druryEf81+svPjP6Y8vsjmNkCSb8l6ZnavUSyBvJ8SRc+9uuLmuIXGmJm1i9pmaSX63ZS3+gfzYckvS3psLtP+TWRtFvSn0j6v9qNRLIGso3x76b8f+UxNjO7VdK3JG1x95/W7qc2d7/u7kslLZD0gJl9vnZPNZnZb0t6293TPxU5ayBflHTXx369QNIblXpBYmb2GY2E8Tfc/du1+8nE3a9KOiL+7uEhSV8xs2GNfPz5BTP7m7otjS1rIP+rpF81s4VmNkPSVyV9t3JPSMbMTNKzkk65+5/X7icDM5tnZrNGfz5T0hpJP6rbVV3u/qfuvsDd+zWSJS+5+9crtzWmlIHs7h9K+kNJ/6CRv6jZ7+6v1e2qLjN7XtI/SVpkZhfN7Pdr95TAQ5J+TyN3PEOjP75cu6nKflnSoJn9m0ZubA67e9oxL3wSG9QDQBIp75ABYCoikAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgiendvHju3Lne39/f9UFOnz4d1m+66aZirc3xxmN4eFiXL1+2Tl/fdk2aRGt2/fr1Ym3x4sUT3oskHT9+/LK7z+vktW3X5K233grr0fd99erVYu39998v1qZNmxYe87777ivWhoaGOl4Tqf26XLhwIaxH3/ucOXOKtTvuuKNYa1qXkl5dP2fOnAnr0bmyaNGiro83Xp1eP10Fcn9/v44dO9Z1M6tWrWr8uiUDAwNdH288VqxY0dXr265Jk2jNogtwMnqRJDM73+lr267J7t27w3r0fR88eLBYO3nyZLF26623hsccHBws1mbPnt3xmkjt12XLli1hPfreN2zY0Orrzpo1q7GvsfTq+lm3bl1Yj86VI0eOdH288er0+uEjCwBIgkAGgCQIZABIgkAGgCQIZABIoqspi7aGh4fD+tGjR4u1ffv2FWt3331362PWdujQobAerclTTz010e3cEKK/+Y8mNKJa9LfxTcfslaGhodbvjaaUommDGpMInxZdw03XT8SsPJW3ZMmSYm08vw+d4g4ZAJIgkAEgCQIZAJIgkAEgCQIZAJIgkAEgiZ6MvTWNDp0/X953o6+vr1hruwFPJz1NtvGMrjVtrHKjatpEJ7Jjx45iLRqfyjDe1WTp0qVhve3mXNE10LQuTRuGTYSmaziycuXKYi1ar9rnA3fIAJAEgQwASRDIAJAEgQwASRDIAJAEgQwASRDIAJBET+aQm54qGz2E8tq1a8VaNJ9Ze864SdOMZbQNYNNcamaTteVj0wNSS6IHhErxQ0J7pamHZcuWFWvRDHZ0jfT6ae8T3UP0+xrN8Y9n9nkicIcMAEkQyACQBIEMAEkQyACQBIEMAEkQyACQRE/G3ppGi6Jxp+hJr1u3bm3b0ri2epwITeM10chPNOIVjfRkH2Vqeqpv27G46PzrxTaS4zWeUazo6eXnzp0r1jKcK9FYXjQWKkmzZ88u1jZv3lysRedg05PsJ2LNuEMGgCQIZABIgkAGgCQIZABIgkAGgCQIZABIoidjb00mY/SoaUSltqYRmWhcKRqDikYBT5w4ER6zF7vIRd9303ikmbV6740w2haNW61evTp8b/QE8+g6iEYkm34vao/FNY1IRvW253nTqGzTmnWCO2QASIJABoAkCGQASIJABoAkCGQASIJABoAkejL2dujQobDe19dXrO3YsaPVMaORngyaHlwZja9FI0fRmFPTWE7th6c2jRVF58nKlSsnup2ein5Po+9bitctOh+ih6MODAyEx2x7XfZKdC5H6xV93xMx1taEO2QASIJABoAkCGQASIJABoAkCGQASIJABoAkCGQASKInc8iDg4Nhfc+ePa2+7vr164u17FsuNs0hR/Oj0axk9H1nn81ueqr0vn37irXoCcU3gqj/pnM5esJyNMO8du3aYq32U9mbNPUXbb8ZbV8bnYO9mNPnDhkAkiCQASAJAhkAkiCQASAJAhkAkiCQASAJc/fOX2z2jqTzk9dOCne7+7xOXzxF1kTqYl1Yk7FNkXVhTcbW0bp0FcgAgMnDRxYAkASBDABJEMgAkASBDABJEMgAkASBDABJEMgAkASBDABJpA1kMxs2s383syEzO1a7nwzMbJaZvWBmPzKzU2b267V7qsnMFo2eHx/9+KmZ5d5ZvQfMbKuZvWZmr5rZ82b22do9ZWBmm0fX5LWs50na/1PPzIYlrXD3y7V7ycLM9kn6gbs/Y2YzJN3s7uXHH0whZjZN0iVJv+buU+F/xR2Tmc2X9ENJi939fTPbL+nv3X2gbmd1mdnnJX1T0gOSPpD0PUl/4O7/WbWxT0l7h4xPMrPbJD0s6VlJcvcPCONP+KKk16dyGH/MdEkzzWy6pJslvVG5nww+J+mf3f1/3P1DSUcl/U7lnn5O5kB2Sd83s+NmtrF2MwncI+kdSc+Z2Qkze8bMbqndVCJflfR87SZqc/dLkv5M0o8l/UTSNXf/ft2uUnhV0sNmNsfMbpb0ZUl3Ve7p52QO5IfcfbmkL0l60swert1QZdMlLZf0l+6+TNJ/S9pet6UcRj+++YqkA7V7qc3MZktaK2mhpDsl3WJmX6/bVX3ufkrSLkmHNfJxxUlJH1ZtagxpA9nd3xj959uSvqORz36msouSLrr7y6O/fkEjAY2R/2i/4u5v1W4kgTWSzrn7O+7+M0nflvQblXtKwd2fdffl7v6wpCuSUn1+LCUNZDO7xcx+8aOfS/pNjfyRY8py9zclXTCzRaP/6ouS/qNiS5l8TXxc8ZEfS3rQzG42M9PIeXKqck8pmNkvjf7zVyT9rhKeM9NrN1Bwh6TvjJxPmi7pb939e3VbSuGPJH1j9I/oZyU9Ubmf6kY/D3xE0qbavWTg7i+b2QuSXtHIH8lPSPqrul2l8S0zmyPpZ5KedPd3azf0aWnH3gBgqkn5kQUATEUEMgAkQSADQBIEMgAkQSADQBIEMgAkQSADQBIEMgAk8f+8M2css42nKwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vilizating the data\n",
    "for i in range(1,11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n",
    "    plt.text(3,10,str(digits.target[i-1]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training set and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformulate the label. \n",
    "# If the digit is smaller than 5, the label is 0.\n",
    "# If the digit is larger than 5, the label is 1.\n",
    "\n",
    "y_train[y_train < 5 ] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64)\n",
      "(450, 64)\n",
      "(1347,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Architecture of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathematical expression of the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one example $x^{(i)}$:   \n",
    " $$ z^{(i)} = w^T * x^{(i)} +b $$   \n",
    " $$ y^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$   \n",
    " $$L(a^{(i)},y^{(i)}) = -y^{(i)} log(a^{(i)})-(1-y^{(i)})log(1-a^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total cost over all training examples:\n",
    "$$ J = \\frac{1}{m}\\sum_{i=1}^{m}L(a^{(i)},y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Building the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1- Activation function    \n",
    "###### Exercise:\n",
    "Finish the sigmoid funciton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(z):\n",
    "    '''\n",
    "    Compute the sigmoid of z\n",
    "    Arguments: z -- a scalar or numpy array of any size.\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    '''\n",
    "    s = None\n",
    "    s = 1.0 / (1 + np.exp(-1 * z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0,2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "# Test your code \n",
    "# The result should be [0.5 0.88079708]\n",
    "print(\"sigmoid([0,2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1-Initializaing parameters\n",
    "###### Exercise:\n",
    "Finishe the initialize_parameters function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random innitialize the parameters\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    '''\n",
    "    Argument: dim -- size of the w vector\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim,1)\n",
    "    b -- initializaed scalar\n",
    "    '''\n",
    "    \n",
    "    w = None\n",
    "    b = None\n",
    "    w = np.random.randn(dim, 1)\n",
    "    b = np.random.rand()\n",
    "    \n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b,float) or isinstance(b,int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3-Forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some mathematical expressions\n",
    "Forward Propagation:   \n",
    ". X    \n",
    ". A = $\\sigma(w^T*X+b) = (a^{(1)},a^{(2)},...,a^{(m)}$   \n",
    ". J = $-\\frac{1}{m} \\sum_{i=1}^{m}y^{(i)}log(a^{(i)}+(1-y^{(i)})log(1-a^{(i)})$       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some derivative: \n",
    "$$\\frac{\\partial{J}}{\\partial{w}} = \\frac{1}{m}X*(A-Y)^T$$   \n",
    "$$\\frac{\\partial{J}}{\\partial{b}} = \\frac{1}{m}\\sum_{i=1}^m(a^{(i)}-y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Finish the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w,b,X,Y):\n",
    "    '''\n",
    "    Implement the cost function and its gradient for the propagation\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    A = None\n",
    "    cost = None\n",
    "    A = sigmoid(np.dot(w.T, X)+b)\n",
    "    cost = log_loss(Y[0], A[0])\n",
    "    \n",
    "    dw = None\n",
    "    db = None\n",
    "    dw = np.dot(X, (A-Y).T) / m\n",
    "    db = np.sum([A-Y]) / m\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 -Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Minimizing the cost function using gradient descent.   \n",
    "$$\\theta = \\theta - \\alpha*d\\theta$$ where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "#         grads, cost = None\n",
    "        grads, cost = propagate(w,b,X,Y)\n",
    "        \n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "#         w = None\n",
    "#         b = None\n",
    "        w += (-1 * learning_rate * dw)\n",
    "        b += (-1 * learning_rate * db)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise\n",
    "The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function.    \n",
    "Two steps to finish this task:   \n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T*X+b)$   \n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "#     A = None\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "#         None \n",
    "        Y_prediction[0][i] = 0 if A[0][i]<0.5 else 1 \n",
    "    \n",
    "    assert(Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5- Merge all functions into a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations !! You have finished all the necessary components for constructing a model. Now, Let's take the challenge to merge all the implemented function into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "    \n",
    "    # process parameters\n",
    "    X = X_train.T\n",
    "    Y = Y_train.reshape(1, -1)\n",
    "    N = X.shape[0]\n",
    "    w, b = initialize_parameters(N)\n",
    "\n",
    "    # carry iteration\n",
    "    params, grads, costs = optimize(w, b, X, Y, num_iterations=num_iterations,learning_rate=learning_rate, print_cost=print_cost)\n",
    "\n",
    "    # output\n",
    "    final_w= params['w']\n",
    "    final_b = params['b']\n",
    "    Y_predict = predict(final_w, final_b, X_test.T)\n",
    "    test_accuracy = accuracy_score(Y_test, Y_predict[0])\n",
    "\n",
    "    final_d = {\n",
    "        'w': final_w,\n",
    "        'b': final_b,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'cost': costs,\n",
    "    }\n",
    "\n",
    "    return final_d\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1347)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = model(X_train, y_train, X_test, y_test, num_iterations=10000, learning_rate=1e-3, print_cost=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': array([[ 0.48911677],\n",
       "        [-0.4497674 ],\n",
       "        [-0.97057582],\n",
       "        [ 0.54615398],\n",
       "        [ 0.09259061],\n",
       "        [ 0.75731179],\n",
       "        [ 0.47227535],\n",
       "        [ 1.42710524],\n",
       "        [-0.39633252],\n",
       "        [-0.26950789],\n",
       "        [ 0.13391163],\n",
       "        [ 0.26715617],\n",
       "        [ 0.18466083],\n",
       "        [-0.44158625],\n",
       "        [-0.31598342],\n",
       "        [-0.12393648],\n",
       "        [-2.55973928],\n",
       "        [ 0.39750256],\n",
       "        [ 0.86938255],\n",
       "        [-0.30254052],\n",
       "        [-0.97727575],\n",
       "        [-0.04825601],\n",
       "        [-0.21487333],\n",
       "        [ 0.39216246],\n",
       "        [-0.95893012],\n",
       "        [-1.74078761],\n",
       "        [ 0.0244092 ],\n",
       "        [ 0.86730774],\n",
       "        [ 0.11638098],\n",
       "        [ 0.66603122],\n",
       "        [-0.82061281],\n",
       "        [ 0.65011466],\n",
       "        [-0.76665432],\n",
       "        [-0.36407708],\n",
       "        [ 0.40026596],\n",
       "        [ 0.2686487 ],\n",
       "        [-0.02553599],\n",
       "        [-0.4466209 ],\n",
       "        [ 0.49768675],\n",
       "        [-0.20174878],\n",
       "        [-0.38281341],\n",
       "        [ 0.6560904 ],\n",
       "        [-0.47762728],\n",
       "        [ 0.13847442],\n",
       "        [ 0.02160661],\n",
       "        [ 0.45574462],\n",
       "        [-0.52660227],\n",
       "        [-0.64515857],\n",
       "        [-1.62339243],\n",
       "        [-1.95217015],\n",
       "        [ 0.50805792],\n",
       "        [-0.55114831],\n",
       "        [-0.7582584 ],\n",
       "        [-0.52070183],\n",
       "        [ 1.15687851],\n",
       "        [-2.13395451],\n",
       "        [-0.23767353],\n",
       "        [ 0.49359369],\n",
       "        [ 0.77855894],\n",
       "        [-0.4613881 ],\n",
       "        [-0.4534448 ],\n",
       "        [-0.02015997],\n",
       "        [-1.18162619],\n",
       "        [ 1.35161823]]),\n",
       " 'b': 0.26016924093134913,\n",
       " 'test_accuracy': 0.8511111111111112,\n",
       " 'cost': [16.67283103899654,\n",
       "  8.106060611730978,\n",
       "  6.924948369720898,\n",
       "  6.065816318147496,\n",
       "  5.379325647213542,\n",
       "  4.8455408019467985,\n",
       "  4.413788184239838,\n",
       "  4.043288809288379,\n",
       "  3.732566501537512,\n",
       "  3.4742900276520885,\n",
       "  3.2577009383174844,\n",
       "  3.0721441301185535,\n",
       "  2.905241220710737,\n",
       "  2.7531609275853013,\n",
       "  2.617199391005672,\n",
       "  2.4941214369477747,\n",
       "  2.3813615372750023,\n",
       "  2.2781749806331653,\n",
       "  2.1835640539407777,\n",
       "  2.0964303806236546,\n",
       "  2.0150485250313928,\n",
       "  1.9401638790438434,\n",
       "  1.8713122680018823,\n",
       "  1.80713941541986,\n",
       "  1.7481786461068516,\n",
       "  1.694389623734142,\n",
       "  1.6452605383517231,\n",
       "  1.5998800638096875,\n",
       "  1.5584447349780148,\n",
       "  1.5204938338743212,\n",
       "  1.485554653341487,\n",
       "  1.4532299884150996,\n",
       "  1.423175495804708,\n",
       "  1.3948650947686305,\n",
       "  1.3680565442239359,\n",
       "  1.3428115170820727,\n",
       "  1.319000516469623,\n",
       "  1.2964697588994636,\n",
       "  1.2750917690152663,\n",
       "  1.2543726012613763,\n",
       "  1.2345415761529484,\n",
       "  1.215634882189845,\n",
       "  1.197605452335119,\n",
       "  1.180414154323997,\n",
       "  1.1640253599799326,\n",
       "  1.1484036777730933,\n",
       "  1.1335123068342512,\n",
       "  1.1193126430955922,\n",
       "  1.1057648138461325,\n",
       "  1.0928286083481393,\n",
       "  1.080464465519142,\n",
       "  1.0686343389457664,\n",
       "  1.0573022964924534,\n",
       "  1.0464348728741013,\n",
       "  1.0360011782085934,\n",
       "  1.0259728276869278,\n",
       "  1.016323765655924,\n",
       "  1.0070300405816017,\n",
       "  0.9980695813069478,\n",
       "  0.9894219938146167,\n",
       "  0.9810683870814011,\n",
       "  0.9729912168850867,\n",
       "  0.9651741560438271,\n",
       "  0.9576019800708478,\n",
       "  0.9502604682377639,\n",
       "  0.9431363238799098,\n",
       "  0.9362171117815039,\n",
       "  0.929491206132147,\n",
       "  0.9229477492767864,\n",
       "  0.9165766220253564,\n",
       "  0.9103684085840935,\n",
       "  0.904314366842248,\n",
       "  0.8984063968946815,\n",
       "  0.8926370008704787,\n",
       "  0.8869992446536802,\n",
       "  0.8814867140921295,\n",
       "  0.8760934690536128,\n",
       "  0.8708139985157236,\n",
       "  0.8656431707822543,\n",
       "  0.8605761939557753,\n",
       "  0.8556085685701372,\n",
       "  0.8507360553126418,\n",
       "  0.845954640104558,\n",
       "  0.8412605093453043,\n",
       "  0.8366500297939276,\n",
       "  0.8321197324529858,\n",
       "  0.8276663033114205,\n",
       "  0.8232865777576277,\n",
       "  0.8189775359757311,\n",
       "  0.8147363024840895,\n",
       "  0.8105601442503697,\n",
       "  0.8064464700536442,\n",
       "  0.8023928295633401,\n",
       "  0.7983969084681788,\n",
       "  0.7944565300169703,\n",
       "  0.7905696465783404,\n",
       "  0.7867343371512757,\n",
       "  0.7829488019407648,\n",
       "  0.779211355468775,\n",
       "  0.775520420827439]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.选做题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on building your first logistic regression model. It is your time to analyze it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Observe the effect of learning rate on the leraning process.   \n",
    "Hits: plot the learning curve with different learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Observe the effect of iteration_num on the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge ! ! !\n",
    "\n",
    "The original data have images labeled 0,1,2,3,4,5,6,7,8,9. In our logistic model, we only detect if the digit in the image is larger or smaller than 5. Now, Let's go for a more challenging problem. Try to use softmax function to build a model to recognize which digits (0,1,2,3,4,5,6,7,8,9) is in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations ! You have completed assigment 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
